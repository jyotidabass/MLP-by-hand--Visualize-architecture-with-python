{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXUidjl2hfQ4bpDuBZ39YN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyotidabass/MLP-by-hand--Visualize-architecture-with-python/blob/main/MLP_by_hand.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **HANDS ON MLP**\n",
        "\n"
      ],
      "metadata": {
        "id": "1hGSLQfxwcU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get started, we'll install and import the NumPy library, which we'll use for our calculations. Next, we'll create two input data points (X1 and X2) and store them in a 2D array called \"inputs.\"\n",
        "\n",
        "Then, we'll define the weights (w1) and biases (b1) for the first hidden layer.\n",
        "\n",
        "\n",
        "After that, we'll perform a dot product of the inputs and the first hidden layer's weights (w1), add the biases (b1), and store the result in z1. We'll then apply the ReLU activation function to the result (z1) and print it as Out1. This process helps us prepare the data for the next hidden layer."
      ],
      "metadata": {
        "id": "coxFasOwvgSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "import numpy as np\n",
        "\n",
        "# Input data\n",
        "X1 = np.array([2, 1, 2, 1])\n",
        "X2 = np.array([-1, 1, 1, 2])\n",
        "inputs = np.array([X1, X2])\n",
        "# Weights and biases for each hidden layer\n",
        "w1 = np.array([[1, 0, 1], [0, 1, -1], [-1, 1, 0], [1, 0, 0]])\n",
        "b1 = np.array([1, 1, 0])\n",
        "\n",
        "z1 = np.dot(inputs, w1) + b1\n",
        "\n",
        "# Define the ReLU activation function\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "Out1= relu(z1)\n",
        "print(\"Output: \", Out1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09vRg5ZjN8Lf",
        "outputId": "e459171b-d2d7-40fc-9271-41eb8209f14d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Output:  [[2 4 1]\n",
            " [1 3 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll define the weights (w2) and biases (b2) for the second hidden layer. The dot product of the output of the first hidden layer (Out1) and the weights (w2) are computed and the biases (b2) are added.\n",
        "\n",
        "This result is passed through the ReLU activation function and assigned to the variable Out2. Finally, the output of the second hidden layer is printed."
      ],
      "metadata": {
        "id": "tNlic_QWwE2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2 = np.array([[0, -1, 0, -1], [-1, 1, 1, 0], [2, 0, 1, 0]])\n",
        "b2 = np.array([1, 2, 1, 3])\n",
        "z2 = np.dot(Out1, w2) + b2\n",
        "Out2= relu(z2)\n",
        "print(\"Output: \", Out2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PvED8YPvmN0",
        "outputId": "246c4f96-73e1-451e-e153-8d196599e334"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:  [[0 4 6 1]\n",
            " [0 4 4 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll define the weights (w3) and biases (b3) for the third hidden layer. The dot product of the output of the second hidden layer (Out2) and the weights (w3) are calculated, the biases (b3) are added, and the result is stored in z3.\n",
        "\n",
        "Then, the ReLU activation function is applied to z3 to obtain the output of the third hidden layer (Out3)."
      ],
      "metadata": {
        "id": "Tq73aN_4wObL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w3 = np.array([[0, -1, -1], [-1, 1, 0], [-1, 1, 1], [2, 0, 0]])\n",
        "b3 = np.array([5, 2, 3])\n",
        "z3 = np.dot(Out2, w3) + b3\n",
        "Out3= relu(z3)\n",
        "print(\"Output: \", Out3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEzF6MH_wJ_l",
        "outputId": "4bbf697b-9e3e-4e0e-b855-09b95b779d09"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:  [[ 0 12  9]\n",
            " [ 1 10  7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll follow a similar process for the fourth hidden layer. We'll define the weights (w4) and bias (b4) for the output layer. It then calculates the dot product of the output of the third hidden layer (Out3) and the weights (w4), adds the bias (b4), and stores the result in z4.\n",
        "\n",
        "The ReLU activation function is used on z4 to obtain the output of the output layer (Out4), which is then printed.\n",
        "\n",
        "\n",
        "Next, we defined the sigmoid activation function, which takes a value x as input and returns the result of the sigmoid function applied to that value. Then, the sigmoid activation function is applied to the output of the output layer (Out4) and stored in the variable Z. Finally, the resulting value is printed as the \"Final Output\"."
      ],
      "metadata": {
        "id": "Q8cVjNP2wWrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w4 = np.array([-1, 1, -1])\n",
        "b4 = np.array([-2])\n",
        "\n",
        "z4 = np.dot(Out3, w4) + b4\n",
        "Out4= relu(z4)\n",
        "\n",
        "print(\"Output: \", Out4)\n",
        "\n",
        "# Define the sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "Z= sigmoid(Out4)\n",
        "print(\"Final Output: \",Z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCftU--cwSDt",
        "outputId": "c13c9322-3289-4cae-e111-d9b4805b1d15"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:  [1 0]\n",
            "Final Output:  [0.73105858 0.5       ]\n"
          ]
        }
      ]
    }
  ]
}